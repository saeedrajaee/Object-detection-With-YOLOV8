{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca75c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from skimage.morphology import opening, disk, closing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a164d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_Path = r'3.jpg'\n",
    "Image_Path1 = Image_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b737ba0",
   "metadata": {},
   "source": [
    "# Without Increase Google Image Quality and Resolution And Rerun Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026eee15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\s.rajaei\\Desktop\\YOLOV8\\3.jpg: 192x224 3 builds, 426.1ms\n",
      "Speed: 2.0ms preprocess, 426.1ms inference, 1.5ms postprocess per image at shape (1, 3, 192, 224)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[6.9412e+02, 2.8352e+02, 9.3075e+02, 5.4558e+02, 8.5603e-01, 0.0000e+00],\n",
      "        [4.1653e+02, 3.2453e+02, 4.9498e+02, 3.9902e+02, 5.2254e-01, 0.0000e+00],\n",
      "        [9.7259e+02, 5.7762e+02, 1.0255e+03, 6.2050e+02, 3.1346e-01, 0.0000e+00]])\n",
      "cls: tensor([0., 0., 0.])\n",
      "conf: tensor([0.8560, 0.5225, 0.3135])\n",
      "data: tensor([[6.9412e+02, 2.8352e+02, 9.3075e+02, 5.4558e+02, 8.5603e-01, 0.0000e+00],\n",
      "        [4.1653e+02, 3.2453e+02, 4.9498e+02, 3.9902e+02, 5.2254e-01, 0.0000e+00],\n",
      "        [9.7259e+02, 5.7762e+02, 1.0255e+03, 6.2050e+02, 3.1346e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (947, 1279)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[812.4330, 414.5497, 236.6284, 262.0559],\n",
      "        [455.7564, 361.7735,  78.4569,  74.4866],\n",
      "        [999.0565, 599.0604,  52.9365,  42.8889]])\n",
      "xywhn: tensor([[0.6352, 0.4378, 0.1850, 0.2767],\n",
      "        [0.3563, 0.3820, 0.0613, 0.0787],\n",
      "        [0.7811, 0.6326, 0.0414, 0.0453]])\n",
      "xyxy: tensor([[ 694.1188,  283.5218,  930.7471,  545.5776],\n",
      "        [ 416.5280,  324.5302,  494.9849,  399.0168],\n",
      "        [ 972.5883,  577.6159, 1025.5248,  620.5048]])\n",
      "xyxyn: tensor([[0.5427, 0.2994, 0.7277, 0.5761],\n",
      "        [0.3257, 0.3427, 0.3870, 0.4213],\n",
      "        [0.7604, 0.6099, 0.8018, 0.6552]])\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO(r\"best.pt\")\n",
    "\n",
    "# Run inference on an image\n",
    "results = model.predict(Image_Path) # results list\n",
    "\n",
    "# View results\n",
    "for r in results:\n",
    "    print(r.boxes)\n",
    "  # print the Boxes object containing the detection bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023c59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(Image_Path)\n",
    "bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "count = 0\n",
    "array = ['build','pool']\n",
    "for r in results:\n",
    "    for box in r.boxes:\n",
    "        textString = \"\"\n",
    "#         Extract object class and confidence score\n",
    "        score = box.conf.item() * 100\n",
    "        class_id = int(box.cls.item())\n",
    "        x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "            # Print detection info\n",
    "        class_id = box.cls[0].item()\n",
    "        class_id = round(class_id)\n",
    "        \n",
    "#         print(array[class_id])\n",
    "        prob = round(box.conf[0].item(), 2)\n",
    "#         textString += f\"{class_id}\"\n",
    "        textString += f\"{array[class_id]}\"\n",
    "        prob = prob * 100\n",
    "        textString += f\" {prob}%\"\n",
    "\n",
    "        # Calculate font scale based on object size\n",
    "        font = cv2.FONT_HERSHEY_COMPLEX\n",
    "        fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "        fontThickness = 1\n",
    "        textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "        # Draw bounding box, a centroid and label on the image\n",
    "        img = cv2.rectangle(img, (x1,y1), (x2,y2), color = (255,0,0),  thickness=bbx_thickness)\n",
    "        center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
    "\n",
    "#         img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)    \n",
    "        if textString != \"\":\n",
    "            if (y1 < textSize[1]):\n",
    "                y1 = y1 + textSize[1]\n",
    "            else:\n",
    "                y1 -= 2\n",
    "            # show the details text in a filled rectangle\n",
    "#             img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), color = (255,0,0), cv2.cv.CV_FILLED == -1)\n",
    "            img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), color = (255,0,0))\n",
    "\n",
    "            img = cv2.putText(img, textString , \n",
    "                (x1, y1), font, \n",
    "                fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06af6b",
   "metadata": {},
   "source": [
    "# Increase Google Image Quality and Resolution And Rerun Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09713533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://github.com/AKASH2907/project_sunroof_india\n",
    "# \n",
    "def auto_canny(image, sigma=0.33):\n",
    "    # compute the median of the single channel pixel intensities\n",
    "    v = np.median(image)\n",
    "\n",
    "    # apply automatic Canny edge detection using the computed median\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    edged = cv2.Canny(image, lower, upper)\n",
    "\n",
    "    # return the edged image\n",
    "    return edged\n",
    "\n",
    "img = cv2.imread(Image_Path)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "imgBGR2RGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "blur = cv2.bilateralFilter(imgBGR2RGB, 5, 75, 75)\n",
    "kernel_sharp = np.array((\n",
    "         [-2, -2, -2],\n",
    "         [-2, 17, -2],\n",
    "         [-2, -2, -2]), dtype='int')\n",
    "im = cv2.filter2D(blur, -1, kernel_sharp)\n",
    "\n",
    "# cv2.imshow(\"Image\", im)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a09e4682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 192x224 11 builds, 2 Pools, 382.7ms\n",
      "Speed: 0.0ms preprocess, 382.7ms inference, 1.0ms postprocess per image at shape (1, 3, 192, 224)\n",
      "WARNING  'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "boxes: tensor([[4.2326e+02, 3.3258e+02, 4.8732e+02, 3.9648e+02, 7.0933e-01, 0.0000e+00],\n",
      "        [9.6899e+02, 5.7611e+02, 1.0219e+03, 6.1277e+02, 5.0456e-01, 0.0000e+00],\n",
      "        [7.4478e+02, 2.7934e+02, 8.6886e+02, 4.3085e+02, 4.7591e-01, 0.0000e+00],\n",
      "        [6.8729e+02, 3.8085e+02, 7.5092e+02, 4.4359e+02, 4.2183e-01, 0.0000e+00],\n",
      "        [7.7711e+02, 4.8186e+02, 8.3364e+02, 5.2365e+02, 4.1418e-01, 0.0000e+00],\n",
      "        [6.8254e+02, 4.8994e+02, 7.3336e+02, 5.3375e+02, 4.0773e-01, 0.0000e+00],\n",
      "        [7.5301e+02, 3.1051e+02, 8.6951e+02, 5.1110e+02, 3.7797e-01, 0.0000e+00],\n",
      "        [4.1717e+02, 3.4005e+02, 4.8365e+02, 4.0962e+02, 3.5820e-01, 0.0000e+00],\n",
      "        [6.6958e+02, 4.9946e+02, 7.3400e+02, 5.4514e+02, 3.3948e-01, 0.0000e+00],\n",
      "        [8.9484e+02, 5.7963e+02, 9.4409e+02, 6.1726e+02, 3.1512e-01, 0.0000e+00],\n",
      "        [2.4148e+02, 1.8123e+01, 5.1768e+02, 3.8194e+02, 2.8730e-01, 1.0000e+00],\n",
      "        [6.5161e+02, 5.0244e+02, 7.2513e+02, 5.4808e+02, 2.7033e-01, 0.0000e+00],\n",
      "        [5.7475e+02, 6.8729e+02, 6.2625e+02, 7.4727e+02, 2.6411e-01, 1.0000e+00]])\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.])\n",
      "conf: tensor([0.7093, 0.5046, 0.4759, 0.4218, 0.4142, 0.4077, 0.3780, 0.3582, 0.3395, 0.3151, 0.2873, 0.2703, 0.2641])\n",
      "data: tensor([[4.2326e+02, 3.3258e+02, 4.8732e+02, 3.9648e+02, 7.0933e-01, 0.0000e+00],\n",
      "        [9.6899e+02, 5.7611e+02, 1.0219e+03, 6.1277e+02, 5.0456e-01, 0.0000e+00],\n",
      "        [7.4478e+02, 2.7934e+02, 8.6886e+02, 4.3085e+02, 4.7591e-01, 0.0000e+00],\n",
      "        [6.8729e+02, 3.8085e+02, 7.5092e+02, 4.4359e+02, 4.2183e-01, 0.0000e+00],\n",
      "        [7.7711e+02, 4.8186e+02, 8.3364e+02, 5.2365e+02, 4.1418e-01, 0.0000e+00],\n",
      "        [6.8254e+02, 4.8994e+02, 7.3336e+02, 5.3375e+02, 4.0773e-01, 0.0000e+00],\n",
      "        [7.5301e+02, 3.1051e+02, 8.6951e+02, 5.1110e+02, 3.7797e-01, 0.0000e+00],\n",
      "        [4.1717e+02, 3.4005e+02, 4.8365e+02, 4.0962e+02, 3.5820e-01, 0.0000e+00],\n",
      "        [6.6958e+02, 4.9946e+02, 7.3400e+02, 5.4514e+02, 3.3948e-01, 0.0000e+00],\n",
      "        [8.9484e+02, 5.7963e+02, 9.4409e+02, 6.1726e+02, 3.1512e-01, 0.0000e+00],\n",
      "        [2.4148e+02, 1.8123e+01, 5.1768e+02, 3.8194e+02, 2.8730e-01, 1.0000e+00],\n",
      "        [6.5161e+02, 5.0244e+02, 7.2513e+02, 5.4808e+02, 2.7033e-01, 0.0000e+00],\n",
      "        [5.7475e+02, 6.8729e+02, 6.2625e+02, 7.4727e+02, 2.6411e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (947, 1279)\n",
      "shape: torch.Size([13, 6])\n",
      "xywh: tensor([[455.2894, 364.5261,  64.0524,  63.9016],\n",
      "        [995.4616, 594.4396,  52.9465,  36.6547],\n",
      "        [806.8214, 355.0959, 124.0771, 151.5040],\n",
      "        [719.1025, 412.2183,  63.6328,  62.7389],\n",
      "        [805.3750, 502.7546,  56.5308,  41.7851],\n",
      "        [707.9488, 511.8458,  50.8165,  43.8111],\n",
      "        [811.2595, 410.8039, 116.4924, 200.5900],\n",
      "        [450.4074, 374.8350,  66.4814,  69.5691],\n",
      "        [701.7902, 522.2999,  64.4224,  45.6833],\n",
      "        [919.4608, 598.4407,  49.2493,  37.6313],\n",
      "        [379.5829, 200.0342, 276.1964, 363.8214],\n",
      "        [688.3724, 525.2630,  73.5228,  45.6421],\n",
      "        [600.4998, 717.2798,  51.4997,  59.9744]])\n",
      "xywhn: tensor([[0.3560, 0.3849, 0.0501, 0.0675],\n",
      "        [0.7783, 0.6277, 0.0414, 0.0387],\n",
      "        [0.6308, 0.3750, 0.0970, 0.1600],\n",
      "        [0.5622, 0.4353, 0.0498, 0.0663],\n",
      "        [0.6297, 0.5309, 0.0442, 0.0441],\n",
      "        [0.5535, 0.5405, 0.0397, 0.0463],\n",
      "        [0.6343, 0.4338, 0.0911, 0.2118],\n",
      "        [0.3522, 0.3958, 0.0520, 0.0735],\n",
      "        [0.5487, 0.5515, 0.0504, 0.0482],\n",
      "        [0.7189, 0.6319, 0.0385, 0.0397],\n",
      "        [0.2968, 0.2112, 0.2159, 0.3842],\n",
      "        [0.5382, 0.5547, 0.0575, 0.0482],\n",
      "        [0.4695, 0.7574, 0.0403, 0.0633]])\n",
      "xyxy: tensor([[ 423.2632,  332.5753,  487.3156,  396.4770],\n",
      "        [ 968.9883,  576.1122, 1021.9349,  612.7670],\n",
      "        [ 744.7829,  279.3439,  868.8600,  430.8480],\n",
      "        [ 687.2861,  380.8489,  750.9189,  443.5878],\n",
      "        [ 777.1096,  481.8621,  833.6404,  523.6472],\n",
      "        [ 682.5405,  489.9402,  733.3571,  533.7513],\n",
      "        [ 753.0134,  310.5089,  869.5057,  511.0989],\n",
      "        [ 417.1667,  340.0504,  483.6481,  409.6195],\n",
      "        [ 669.5790,  499.4583,  734.0014,  545.1415],\n",
      "        [ 894.8361,  579.6251,  944.0854,  617.2564],\n",
      "        [ 241.4847,   18.1235,  517.6811,  381.9449],\n",
      "        [ 651.6111,  502.4420,  725.1339,  548.0840],\n",
      "        [ 574.7499,  687.2926,  626.2496,  747.2670]])\n",
      "xyxyn: tensor([[0.3309, 0.3512, 0.3810, 0.4187],\n",
      "        [0.7576, 0.6084, 0.7990, 0.6471],\n",
      "        [0.5823, 0.2950, 0.6793, 0.4550],\n",
      "        [0.5374, 0.4022, 0.5871, 0.4684],\n",
      "        [0.6076, 0.5088, 0.6518, 0.5530],\n",
      "        [0.5337, 0.5174, 0.5734, 0.5636],\n",
      "        [0.5888, 0.3279, 0.6798, 0.5397],\n",
      "        [0.3262, 0.3591, 0.3781, 0.4325],\n",
      "        [0.5235, 0.5274, 0.5739, 0.5757],\n",
      "        [0.6996, 0.6121, 0.7381, 0.6518],\n",
      "        [0.1888, 0.0191, 0.4048, 0.4033],\n",
      "        [0.5095, 0.5306, 0.5670, 0.5788],\n",
      "        [0.4494, 0.7258, 0.4896, 0.7891]])\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO(r\"best.pt\")\n",
    "\n",
    "# Run inference on an image\n",
    "results = model.predict(im) # results list\n",
    "\n",
    "# View results\n",
    "for r in results:\n",
    "    print(r.boxes)\n",
    "  # print the Boxes object containing the detection bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e356e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(Image_Path1)\n",
    "bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "count = 0\n",
    "array = ['build','pool']\n",
    "for r in results:\n",
    "    for box in r.boxes:\n",
    "        textString = \"\"\n",
    "#         Extract object class and confidence score\n",
    "        score = box.conf.item() * 100\n",
    "        class_id = int(box.cls.item())\n",
    "        x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "            # Print detection info\n",
    "        class_id = box.cls[0].item()\n",
    "        class_id = round(class_id)\n",
    "        \n",
    "#         print(array[class_id])\n",
    "        prob = round(box.conf[0].item(), 2)\n",
    "#         textString += f\"{class_id}\"\n",
    "        textString += f\"{array[class_id]}\"\n",
    "        prob = prob * 100\n",
    "        textString += f\" {prob}%\"\n",
    "\n",
    "        # Calculate font scale based on object size\n",
    "        font = cv2.FONT_HERSHEY_COMPLEX\n",
    "        fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "        fontThickness = 1\n",
    "        textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "        # Draw bounding box, a centroid and label on the image\n",
    "        img = cv2.rectangle(img, (x1,y1), (x2,y2), color = (255,0,0),  thickness=bbx_thickness)\n",
    "        center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
    "\n",
    "#         img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)    \n",
    "        if textString != \"\":\n",
    "            if (y1 < textSize[1]):\n",
    "                y1 = y1 + textSize[1]\n",
    "            else:\n",
    "                y1 -= 2\n",
    "            # show the details text in a filled rectangle\n",
    "#             img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), color = (255,0,0), cv2.cv.CV_FILLED == -1)\n",
    "            img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), color = (255,0,0))\n",
    "\n",
    "            img = cv2.putText(img, textString , \n",
    "                (x1, y1), font, \n",
    "                fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0347b881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'build', 1: 'Pool'}\n",
      "orig_img: array([[[ 33,  43,  30],\n",
      "        [ 32,  38,  48],\n",
      "        [ 41,  49,  57],\n",
      "        ...,\n",
      "        [ 36,  28,  24],\n",
      "        [ 33,  21,  23],\n",
      "        [ 39,  27,  29]],\n",
      "\n",
      "       [[ 65,  65,  77],\n",
      "        [ 83,  62,  76],\n",
      "        [ 94,  94,  85],\n",
      "        ...,\n",
      "        [ 39,  48,  42],\n",
      "        [ 30,  37,  39],\n",
      "        [ 34,  41,  24]],\n",
      "\n",
      "       [[109, 116, 113],\n",
      "        [108, 113, 112],\n",
      "        [121, 126, 123],\n",
      "        ...,\n",
      "        [ 52,  53,  51],\n",
      "        [ 54,  53,  57],\n",
      "        [ 54,  51,  38]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[101, 100, 108],\n",
      "        [ 87,  86,  94],\n",
      "        [ 61,  64,  70],\n",
      "        ...,\n",
      "        [ 55,  60,  55],\n",
      "        [ 77,  63,  77],\n",
      "        [ 69,  74,  69]],\n",
      "\n",
      "       [[ 97, 105,  98],\n",
      "        [ 87,  97,  88],\n",
      "        [ 54,  68,  57],\n",
      "        ...,\n",
      "        [ 66,  60,  59],\n",
      "        [ 73,  67,  66],\n",
      "        [ 84,  80,  77]],\n",
      "\n",
      "       [[116, 118, 117],\n",
      "        [ 89,  93,  90],\n",
      "        [ 58,  49,  63],\n",
      "        ...,\n",
      "        [ 64,  62,  63],\n",
      "        [ 69,  67,  68],\n",
      "        [ 63,  61,  62]]], dtype=uint8)\n",
      "orig_shape: (947, 1279)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 0.0, 'inference': 382.68184661865234, 'postprocess': 0.9987354278564453}\n",
      "423 333 487 396 Pool 0.71\n",
      "969 576 1022 613 Pool 0.5\n",
      "745 279 869 431 Pool 0.48\n",
      "687 381 751 444 Pool 0.42\n",
      "777 482 834 524 Pool 0.41\n",
      "683 490 733 534 Pool 0.41\n",
      "753 311 870 511 Pool 0.38\n",
      "417 340 484 410 Pool 0.36\n",
      "670 499 734 545 Pool 0.34\n",
      "895 580 944 617 Pool 0.32\n",
      "241 18 518 382 Pool 0.29\n",
      "652 502 725 548 Pool 0.27\n",
      "575 687 626 747 Pool 0.26\n"
     ]
    }
   ],
   "source": [
    "result = results[0]\n",
    "print(result)\n",
    "output = []\n",
    "for box in result.boxes:\n",
    "    x1, y1, x2, y2 = [\n",
    "       round(x) for x in box.xyxy[0].tolist()\n",
    "    ]\n",
    "#     class_id = box.cls[0].item()\n",
    "    prob = round(box.conf[0].item(), 2)\n",
    "    output.append([\n",
    "    x1, y1, x2, y2, result.names[class_id], prob\n",
    "    ])\n",
    "    print(x1, y1, x2, y2, result.names[class_id], prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60039c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
